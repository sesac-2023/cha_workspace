{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/one-hot-encoding-in-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded vectors for the first sentence:\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the corpus of text\n",
    "corpus = [\n",
    "\t\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\"She sells seashells by the seashore.\",\n",
    "\t\"Peter Piper picked a peck of pickled peppers.\"\n",
    "]\n",
    "\n",
    "# Create a set of unique words in the corpus\n",
    "unique_words = set()\n",
    "for sentence in corpus:\n",
    "\tfor word in sentence.split():\n",
    "\t\tunique_words.add(word.lower())\n",
    "\n",
    "# Create a dictionary to map each\n",
    "# unique word to an index\n",
    "word_to_index = {}\n",
    "for i, word in enumerate(unique_words):\n",
    "\tword_to_index[word] = i\n",
    "\n",
    "# Create one-hot encoded vectors for\n",
    "# each word in the corpus\n",
    "one_hot_vectors = []\n",
    "for sentence in corpus:\n",
    "\tsentence_vectors = []\n",
    "\tfor word in sentence.split():\n",
    "\t\tvector = np.zeros(len(unique_words))\n",
    "\t\tvector[word_to_index[word.lower()]] = 1\n",
    "\t\tsentence_vectors.append(vector)\n",
    "\tone_hot_vectors.append(sentence_vectors)\n",
    "\n",
    "# Print the one-hot encoded vectors\n",
    "# for the first sentence\n",
    "print(\"One-hot encoded vectors for the first sentence:\")\n",
    "for vector in one_hot_vectors[0]:\n",
    "\tprint(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences 1:\n",
      "The: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cat: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "sat: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "on: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "the: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "mat.: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Sentences 2:\n",
      "The: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "dog: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "chased: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "the: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cat.: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Sentences 3:\n",
      "The: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "mat: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "was: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "soft: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "and: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "fluffy.: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the sentences\n",
    "sentences = [\n",
    "\t'The cat sat on the mat.',\n",
    "\t'The dog chased the cat.',\n",
    "\t'The mat was soft and fluffy.'\n",
    "]\n",
    "\n",
    "# Create a vocabulary set\n",
    "vocab = set()\n",
    "for sentence in sentences:\n",
    "\twords = sentence.lower().split()\n",
    "\tfor word in words:\n",
    "\t\tvocab.add(word)\n",
    "\n",
    "# Create a dictionary to map words to integers\n",
    "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Create a binary vector for each word in each sentence\n",
    "vectors = []\n",
    "for sentence in sentences:\n",
    "\twords = sentence.lower().split()\n",
    "\tsentence_vectors = []\n",
    "\tfor word in words:\n",
    "\t\tbinary_vector = np.zeros(len(vocab))\n",
    "\t\tbinary_vector[word_to_int[word]] = 1\n",
    "\t\tsentence_vectors.append(binary_vector)\n",
    "\tvectors.append(sentence_vectors)\n",
    "\n",
    "# Print the one-hot encoded vectors for each word in each sentence\n",
    "for i in range(len(sentences)):\n",
    "\tprint(f\"Sentences {i + 1}:\")\n",
    "\tfor j in range(len(vectors[i])):\n",
    "\t\tprint(f\"{sentences[i].split()[j]}: {vectors[i][j]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'sat', 'on', 'the', 'mat.']\n",
      "the\n",
      "cat\n",
      "sat\n",
      "on\n",
      "the\n",
      "mat.\n",
      "['the', 'dog', 'chased', 'the', 'cat.']\n",
      "the\n",
      "dog\n",
      "chased\n",
      "the\n",
      "cat.\n",
      "['the', 'mat', 'was', 'soft', 'and', 'fluffy.']\n",
      "the\n",
      "mat\n",
      "was\n",
      "soft\n",
      "and\n",
      "fluffy.\n"
     ]
    }
   ],
   "source": [
    "# Define the sentences\n",
    "sentences = [\n",
    "\t'The cat sat on the mat.',\n",
    "\t'The dog chased the cat.',\n",
    "\t'The mat was soft and fluffy.'\n",
    "]\n",
    "\n",
    "# Create a vocabulary set\n",
    "vocab = set()\n",
    "for sentence in sentences:\n",
    "\twords = sentence.lower().split()\n",
    "\tprint(words)\n",
    "\tfor word in words:\n",
    "\t\tprint(word)\n",
    "\t\tvocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and',\n",
       " 'cat',\n",
       " 'cat.',\n",
       " 'chased',\n",
       " 'dog',\n",
       " 'fluffy.',\n",
       " 'mat',\n",
       " 'mat.',\n",
       " 'on',\n",
       " 'sat',\n",
       " 'soft',\n",
       " 'the',\n",
       " 'was'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0,\n",
       " 'on': 1,\n",
       " 'fluffy.': 2,\n",
       " 'cat.': 3,\n",
       " 'mat': 4,\n",
       " 'the': 5,\n",
       " 'chased': 6,\n",
       " 'was': 7,\n",
       " 'sat': 8,\n",
       " 'and': 9,\n",
       " 'soft': 10,\n",
       " 'dog': 11,\n",
       " 'mat.': 12}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "word_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'sat', 'on', 'the', 'mat.']\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "['the', 'dog', 'chased', 'the', 'cat.']\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "['the', 'mat', 'was', 'soft', 'and', 'fluffy.']\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Create a binary vector for each word in each sentence\n",
    "vectors = []\n",
    "for sentence in sentences:\n",
    "\twords = sentence.lower().split()\n",
    "\tprint(words)\n",
    "\tsentence_vectors = []\n",
    "\tfor word in words:\n",
    "\t\tbinary_vector = np.zeros(len(vocab))\n",
    "\t\tbinary_vector[word_to_int[word]] = 1\n",
    "\t\tprint(binary_vector)\n",
    "\t\tsentence_vectors.append(binary_vector)\n",
    "\tvectors.append(sentence_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       "  array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])],\n",
       " [array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       " [array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       "  array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences 1:\n",
      "The: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cat: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "sat: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "on: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "the: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "mat.: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Sentences 2:\n",
      "The: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "dog: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "chased: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "the: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cat.: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Sentences 3:\n",
      "The: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "mat: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "was: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "soft: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "and: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "fluffy.: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Print the one-hot encoded vectors for each word in each sentence\n",
    "for i in range(len(sentences)):\n",
    "\tprint(f\"Sentences {i + 1}:\")\n",
    "\tfor j in range(len(vectors[i])):\n",
    "\t\tprint(f\"{sentences[i].split()[j]}: {vectors[i][j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "The\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cat\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "sat\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "on\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "the\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "mat.\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "2\n",
      "The\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "dog\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "chased\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "the\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cat.\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "3\n",
      "The\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "mat\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "was\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "soft\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "and\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "fluffy.\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)): # 0~2\n",
    "\tprint(i + 1)\n",
    "\tfor j in range(len(vectors[i])): # 0~5\n",
    "\t\tprint(sentences[i].split()[j])\n",
    "\t\tprint(vectors[i][j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
