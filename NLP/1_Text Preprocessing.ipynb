{"cells":[{"cell_type":"markdown","metadata":{"id":"rLGhbEiOoAR7"},"source":["# 텍스트 전처리 (Text Preprocessing)\n","\n","*   텍스트를 자연어 처리를 위해 용도에 맞도록 사전에 표준화 하는 작업\n","*   텍스트 내 정보를 유지하고, 중복을 제거하여 분석 효율성을 높이기 위해 전처리를 수행\n","\n"]},{"cell_type":"markdown","metadata":{"id":"E585k45HDx5E"},"source":["### 1) 토큰화 (Tokenizing)\n","* 텍스트를 자연어 처리를 위해 분리 하는 것을 토큰화라고 함\n","* 토큰화는 단어별로 분리하는 \"단어 토큰화(Word Tokenization)\"와 문장별로 분리하는 \"문장 토큰화(Sentence Tokenization)\"로 구분\n","\n","(이후 실습에서는 단어 토큰화를 \"토큰화\"로 통일하여 칭하도록 한다)"]},{"cell_type":"markdown","metadata":{"id":"senwNSwgDzQc"},"source":["### 2) 품사 부착(PoS Tagging)\n","* 각 토큰에 품사 정보를 추가\n","* 분석시에 불필요한 품사를 제거하거나 (예. 조사, 접속사 등) 필요한 품사를 필터링 하기 위해 사용"]},{"cell_type":"markdown","metadata":{"id":"R15ri5czDyzc"},"source":["### 3) 개체명 인식 (NER, Named Entity Recognition)\n","* 각 토큰의 개체 구분(기관, 인물, 지역, 날짜 등) 태그를 부착\n","* 텍스트가 무엇과 관련되어있는지 구분하기 위해 사용\n","* 예를 들어, 과일의 apple과 기업의 apple을 구분하는 방법이 개체명 인식임"]},{"cell_type":"markdown","metadata":{"id":"Dfq99EkzD1Tk"},"source":["### 4) 원형 복원 (Stemming & Lemmatization)\n","* 각 토큰의 원형 복원을 함으로써 토큰을 표준화하여 불필요한 데이터 중복을 방지 (=단어의 수를 줄일수 있어 연산을 효율성을 높임)\n","* 어간 추출(Stemming) : 품사를 무시하고 규칙에 기반하여 어간을 추출\n","* 표제어 추출 (Lemmatization) : 품사정보를 유지하여 표제어 추출"]},{"cell_type":"markdown","metadata":{"id":"R5HQOjRvDxmd"},"source":["### 5) 불용어 처리 (Stopword)\n","* 자연어 처리를 위해 불필요한 요소를 제거하는 작업\n","* 불필요한 품사를 제거하는 작업과 불필요한 단어를 제거하는 작업으로 구성\n","* 불필요한 토큰을 제거함으로써 연산의 효율성을 높임"]},{"cell_type":"markdown","metadata":{"id":"QaIYJczuaS0n"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KysKAL3VlgQN"},"source":["# 1 영문 전처리 실습\n","\n","\n","NLTK lib (https://www.nltk.org/) 사용"]},{"cell_type":"markdown","metadata":{"id":"mND0us3Jppcu"},"source":["## 1.1 실습용 영문기사 수집\n","온라인 기사를 바로 수집하여 실습데이터로 사용\n","\n","https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":818,"status":"ok","timestamp":1690702898076,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"F4pcegjEqewF"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":919,"status":"ok","timestamp":1690702898993,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"wFUHpRU3qhLu"},"outputs":[],"source":["url = 'https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/?ss=ai-big-data#45dd5dd61f25'\n","response = requests.get(url)\n","soup = BeautifulSoup(response.text,'html.parser')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1690702898994,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"VlX5zJweqn-B"},"outputs":[],"source":["eng_news = soup.select('p') #[class=\"speakable-paragraph\"]')\n","eng_text = eng_news[3].get_text()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1690702898994,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"urFWZplwq5My","outputId":"de130bda-b465-4af4-fa35-588968bbebc8"},"outputs":[{"data":{"text/plain":["\"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\""]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["eng_text"]},{"cell_type":"markdown","metadata":{"id":"yv0ASXb8qa6H"},"source":["## 1.2 영문 토큰화\n","https://www.nltk.org/api/nltk.tokenize.html\n","\n","- nltk 패키지 (자연어처리 패키지 - Natural Language Toolkit)\n","  - tokenize.word_tokenize() : 마침표와 구두점(온점(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호)으로 구분하여 토큰화\n","  - tokenize.WordPunctTokenizer : 알파벳이 아닌 문자를 구분하여 토큰화\n","  - tokenize.TreebankWordTokenizer() : 정규표현식에 기반한 토큰화"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13168,"status":"ok","timestamp":1690702919202,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"ZPZeW4nqTpZD","outputId":"701e3097-82a7-49e0-cf02-be599bbf6a69"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.7)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n","Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n","Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n","Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n","Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3552,"status":"ok","timestamp":1690702922751,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"ywTmZDer4iH-","outputId":"fb90c0f4-b401-48c7-dbe9-569409e3bc0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","text = 'Barack Obama likes fried chicken very much'\n","word_tokens = word_tokenize(text)\n","print(word_tokens)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1690702922752,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"8ecl0DtolnEc","outputId":"bcab8111-b882-4665-efe3-8f5642a6defb"},"outputs":[{"name":"stdout","output_type":"stream","text":["['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","token1 = word_tokenize(eng_text)\n","print(token1)"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690702922752,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"NkMwui2CBtTQ","outputId":"4493d40b-e94c-48cf-ebea-d6b538b78440"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;31mInit signature:\u001b[0m \u001b[0mWordPunctTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mDocstring:\u001b[0m     \n","Tokenize a text into a sequence of alphabetic and\n","non-alphabetic characters, using the regexp ``\\w+|[^\\w\\s]+``.\n","\n","    >>> from nltk.tokenize import WordPunctTokenizer\n","    >>> s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n","    >>> WordPunctTokenizer().tokenize(s)\n","    ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York',\n","    '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n","\u001b[1;31mFile:\u001b[0m           c:\\users\\user\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\regexp.py\n","\u001b[1;31mType:\u001b[0m           ABCMeta\n","\u001b[1;31mSubclasses:\u001b[0m     "]}],"source":["WordPunctTokenizer?"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1690702922752,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"rygb4BNXFd13","outputId":"6bc3e27e-038b-4303-fe36-4e4215ca047b"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"]}],"source":["import nltk\n","from nltk.tokenize import WordPunctTokenizer\n","\n","text = 'Barack Obama likes fried chicken very much'\n","wordpuncttoken = WordPunctTokenizer().tokenize(text)\n","print(wordpuncttoken)"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690702922752,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"UzZ-moT5EzwL","outputId":"ddfa524f-2921-4eb4-cb6d-7b88438055b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;31mInit signature:\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mDocstring:\u001b[0m     \n","The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\n","\n","This tokenizer performs the following steps:\n","\n","- split standard contractions, e.g. ``don't`` -> ``do n't`` and ``they'll`` -> ``they 'll``\n","- treat most punctuation characters as separate tokens\n","- split off commas and single quotes, when followed by whitespace\n","- separate periods that appear at the end of line\n","\n",">>> from nltk.tokenize import TreebankWordTokenizer\n",">>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.'''\n",">>> TreebankWordTokenizer().tokenize(s)\n","['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']\n",">>> s = \"They'll save and invest more.\"\n",">>> TreebankWordTokenizer().tokenize(s)\n","['They', \"'ll\", 'save', 'and', 'invest', 'more', '.']\n",">>> s = \"hi, my name can't hello,\"\n",">>> TreebankWordTokenizer().tokenize(s)\n","['hi', ',', 'my', 'name', 'ca', \"n't\", 'hello', ',']\n","\u001b[1;31mFile:\u001b[0m           c:\\users\\user\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py\n","\u001b[1;31mType:\u001b[0m           ABCMeta\n","\u001b[1;31mSubclasses:\u001b[0m     "]}],"source":["TreebankWordTokenizer?"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690702922752,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"VrvBRJqJlitx","outputId":"a750d15b-5690-43af-9933-3547de690956"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"]}],"source":["import nltk\n","from nltk.tokenize import TreebankWordTokenizer\n","\n","text = 'Barack Obama likes fried chicken very much'\n","treebankwordtoken = TreebankWordTokenizer().tokenize(text)\n","print(treebankwordtoken)"]},{"cell_type":"markdown","metadata":{"id":"8-Z-0Nnysqnq"},"source":["## 1.3 영문 품사 부착 (PoS Tagging)\n","- pos_tag() : 분리한 토큰마다 품사를 부착한다\n","\n","https://www.nltk.org/api/nltk.tag.html\n","\n","태크목록 : https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":344,"status":"ok","timestamp":1690702923094,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"mHWVrEmTlosg","outputId":"e5dc5ade-2cad-4a01-ff31-d9e935d48ea2"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"data":{"text/plain":["True"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from nltk import pos_tag\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690702923094,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"jwtt2LxqlrVS","outputId":"9a4cb7cb-d27b-4651-be44-fc3c88caa53f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('Barack', 'NNP'), ('Obama', 'NNP'), ('likes', 'VBZ'), ('fried', 'VBN'), ('chicken', 'JJ'), ('very', 'RB'), ('much', 'JJ')]\n"]}],"source":["taggedToken = pos_tag(word_tokens)\n","print(taggedToken)"]},{"cell_type":"markdown","metadata":{"id":"lDo-5-khs5Oz"},"source":["## 1.4 개체명 인식 (NER, Named Entity Recognition)\n","- ne_chunk() : 개체명 인식\n","\n","http://www.nltk.org/api/nltk.chunk.html"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":884,"status":"ok","timestamp":1690702923977,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"Clj4X6Gilsi9","outputId":"d3bcc435-1520-44e3-ccdd-97b4460cc058"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package words to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('words')\n","nltk.download('maxent_ne_chunker')"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690702923977,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"VdkMJHO7mBgi","outputId":"ff298411-88ef-428a-8be0-56aad203ca12"},"outputs":[{"name":"stdout","output_type":"stream","text":["(S\n","  (PERSON Barack/NNP)\n","  (ORGANIZATION Obama/NNP)\n","  likes/VBZ\n","  fried/VBN\n","  chicken/JJ\n","  very/RB\n","  much/JJ)\n"]}],"source":["from nltk import ne_chunk\n","neToken = ne_chunk(taggedToken)\n","print(neToken)"]},{"cell_type":"markdown","metadata":{"id":"aHjV0h0ZtM-t"},"source":["## 1.5 원형 복원\n","각 토큰의 원형을 복원하여 표준화 한다."]},{"cell_type":"markdown","metadata":{"id":"r2eCnbChtXjo"},"source":["### 1.5.1 어간추출 (Stemming)\n","\n","* 규칙에 기반 하여 토큰을 표준화\n","* ning제거, ful 제거 등\n","\n","https://www.nltk.org/api/nltk.stem.html\n","\n","규칙상세 : https://tartarus.org/martin/PorterStemmer/def.txt"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690702923977,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"n-AvZXHLmCy2","outputId":"dc099b14-ef04-4587-9b21-d3d67b676a43"},"outputs":[{"name":"stdout","output_type":"stream","text":["running -> run\n","beautiful -> beauti\n","believes -> believ\n","using -> use\n","conversation -> convers\n","organization -> organ\n","studies -> studi\n"]}],"source":["from nltk.stem import PorterStemmer\n","ps = PorterStemmer()\n","\n","print(\"running -> \" + ps.stem(\"running\"))\n","print(\"beautiful -> \" + ps.stem(\"beautiful\"))\n","print(\"believes -> \" + ps.stem(\"believes\"))\n","print(\"using -> \" + ps.stem(\"using\"))\n","print(\"conversation -> \" + ps.stem(\"conversation\"))\n","print(\"organization -> \" + ps.stem(\"organization\"))\n","print(\"studies -> \" + ps.stem(\"studies\"))"]},{"cell_type":"markdown","metadata":{"id":"4haNWIcCtZza"},"source":["### 1.5.2 표제어 추출 (Lemmatization)\n","\n","* 품사정보를 보존하여 토큰을 표준화\n","\n","http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1690702924432,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"MdxBuzdymR7w","outputId":"e1d10303-5f33-46e3-f8d4-43a90d6f8ee8"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1944,"status":"ok","timestamp":1690702926375,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"2mQSzsCZmMBd","outputId":"a4881909-8c3a-4249-babb-fcfacd1aa340"},"outputs":[{"name":"stdout","output_type":"stream","text":["running -> running\n","beautiful -> beautiful\n","believes -> belief\n","using -> using\n","conversation -> conversation\n","organization -> organization\n","studies -> study\n"]}],"source":["from nltk.stem import WordNetLemmatizer\n","wl = WordNetLemmatizer()\n","\n","print(\"running -> \" + wl.lemmatize(\"running\"))\n","print(\"beautiful -> \" + wl.lemmatize(\"beautiful\"))\n","print(\"believes -> \" + wl.lemmatize(\"believes\"))\n","print(\"using -> \" + wl.lemmatize(\"using\"))\n","print(\"conversation -> \" + wl.lemmatize(\"conversation\"))\n","print(\"organization -> \" + wl.lemmatize(\"organization\"))\n","print(\"studies -> \" + wl.lemmatize(\"studies\"))"]},{"cell_type":"markdown","metadata":{"id":"nmY_SvDMb0fz"},"source":["## 1.6 불용어 처리 (Stopword)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690702926375,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"lOUE-BBKcn4S"},"outputs":[],"source":["stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT', 'VBZ','VBP']"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690702926375,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"CyDJ4JiscnrY","outputId":"7dbd826e-bfb4-401e-a11d-cd52a96a1244"},"outputs":[{"data":{"text/plain":["[(('Barack', 'NNP'), 1),\n"," (('Obama', 'NNP'), 1),\n"," (('likes', 'VBZ'), 1),\n"," (('fried', 'VBN'), 1),\n"," (('chicken', 'JJ'), 1),\n"," (('very', 'RB'), 1),\n"," (('much', 'JJ'), 1)]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# 최빈어 조회. 최빈어를 조회하여 불용어 제거 대상을 선정\n","from collections import Counter\n","Counter(taggedToken).most_common()"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1690702926375,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"zNhxqDVkcnX9","outputId":"162776fe-1f54-4ff3-811e-f0b21450c6dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Barack', 'Obama', 'fried', 'chicken', 'very', 'much']\n"]}],"source":["stopWord = [',','be','able']\n","\n","word = []\n","for tag in taggedToken:\n","    if tag[1] not in stopPos:  # 품사 체크\n","        if tag[0] not in stopWord:  # 토큰 체크\n","            word.append(tag[0])\n","\n","print(word)"]},{"cell_type":"markdown","metadata":{"id":"QV0orUsOb6wD"},"source":["## 1.7 영문 텍스트 전처리 종합"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690702926375,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"Pbz6tLP_mNrn","outputId":"58d94814-3d6e-45de-9bdc-ca4882e297b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Obama', 'loves', 'fried', 'chicken', 'of', 'KFC']\n","[('Barack', 'NNP'), ('Obama', 'NNP'), ('likes', 'VBZ'), ('fried', 'VBN'), ('chicken', 'JJ'), ('very', 'RB'), ('much', 'JJ')]\n","(S\n","  (PERSON Barack/NNP)\n","  (ORGANIZATION Obama/NNP)\n","  likes/VBZ\n","  fried/VBN\n","  chicken/JJ\n","  very/RB\n","  much/JJ)\n","loves -> love\n","fried -> fri\n","loves -> love\n","fried -> fried\n","['Obama', 'loves', 'chicken', 'KFC']\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package words to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('words')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('wordnet')\n","\n","\n","from nltk.tokenize import TreebankWordTokenizer\n","sumtoken = TreebankWordTokenizer().tokenize(\"Obama loves fried chicken of KFC\")\n","print(sumtoken)\n","\n","from nltk import pos_tag\n","sumTaggedToken = pos_tag(sumtoken)\n","print(taggedToken)\n","\n","from nltk import ne_chunk\n","sumNeToken = ne_chunk(sumTaggedToken)\n","print(neToken)\n","\n","from nltk.stem import PorterStemmer\n","ps = PorterStemmer()\n","print(\"loves -> \" + ps.stem(\"loves\"))\n","print(\"fried -> \" + ps.stem(\"fried\"))\n","\n","from nltk.stem import WordNetLemmatizer\n","wl = WordNetLemmatizer()\n","print(\"loves -> \" + wl.lemmatize(\"loves\"))\n","print(\"fried -> \" + wl.lemmatize(\"fried\"))\n","\n","#불용어 처리\n","sumStopPos = ['IN']\n","sumStopWord = ['fried']\n","\n","word = []\n","for tag in sumTaggedToken:\n","    if tag[1] not in sumStopPos:\n","        if tag[0] not in sumStopWord:\n","            word.append(tag[0])\n","\n","print(word)"]},{"cell_type":"markdown","metadata":{"id":"BMErzPcbuYEa"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"C0Dhqm4zkHXl"},"source":["# 2 한글 전처리 실습\n","영문은 공백으로 토큰화가 가능하지만, 한글의 경우 품사를 고려하여 토큰화 해야한다."]},{"cell_type":"markdown","metadata":{"id":"w09FHRgIphw5"},"source":["## 2.1 한글 토큰화 및 형태소 분석"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12955,"status":"ok","timestamp":1690702939328,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"Xj3gdRSzhC8n","outputId":"de34f600-406b-4200-aa42-5e5442dc965b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: konlpy in c:\\users\\user\\anaconda3\\lib\\site-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: lxml>=4.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (4.9.1)\n","Requirement already satisfied: numpy>=1.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (22.0)\n"]}],"source":["#konlpy 설치\n","!pip install konlpy"]},{"cell_type":"markdown","metadata":{"id":"5IZWN4xX4HXW"},"source":["한글 자연어처리기 비교\n","\n","https://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221337575742"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13787,"status":"ok","timestamp":1690702953113,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"__e0d_9Svzor","outputId":"43b47cd3-e2ab-481d-c29a-6d02087a2b28"},"outputs":[{"name":"stdout","output_type":"stream","text":["['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"]}],"source":["# 코모란(Komoran) 토큰화\n","from konlpy.tag import Komoran\n","komoran= Komoran()\n","kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n","komoran_tokens = komoran.morphs(kor_text)\n","print(komoran_tokens)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3085,"status":"ok","timestamp":1690702956195,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"0ZD4PsSCeztM","outputId":"721f608a-645e-42a2-e923-c8fdc153b32a"},"outputs":[{"name":"stdout","output_type":"stream","text":["['인간', '이', '컴퓨터', '와', '대화', '하고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"]}],"source":["# 한나눔(Hannanum) 토큰화\n","from konlpy.tag import Hannanum\n","hannanum= Hannanum()\n","kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n","hannanum_tokens = hannanum.morphs(kor_text)\n","print(hannanum_tokens)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15623,"status":"ok","timestamp":1690702971817,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"ORRFr8tHe1VX","outputId":"73af34a5-b4d5-4e76-9e79-a4a050862ba7"},"outputs":[{"name":"stdout","output_type":"stream","text":["['인간', '이', '컴퓨터', '와', '대화', '하고', '있다는', '것', '을', '깨닫지', '못', '하고', '인간', '과', '대화', '를', '계속', '할', '수', '있다면', '컴퓨터', '는', '지능', '적', '인', '것', '으로', '간주', '될', '수', '있습니다', '.']\n"]}],"source":["# Okt 토큰화\n","from konlpy.tag import Okt\n","okt= Okt()\n","kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n","okt_tokens = okt.morphs(kor_text)\n","print(okt_tokens)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18758,"status":"ok","timestamp":1690702990573,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"COrUs_nHe26J","outputId":"ab8f03f0-531f-4828-e2f7-d850bd459015"},"outputs":[{"name":"stdout","output_type":"stream","text":["['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"]}],"source":["# Kkma 토큰화\n","from konlpy.tag import Kkma\n","kkma= Kkma()\n","kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n","kkma_tokens = kkma.morphs(kor_text)\n","print(kkma_tokens)"]},{"cell_type":"markdown","metadata":{"id":"2M7nyptjunTG"},"source":["## 2.2 한글 품사 부착 (PoS Tagging)\n","\n","PoS Tag 목록\n","\n","https://docs.google.com/spreadsheets/u/1/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690702990573,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"2t6txrctj8nC","outputId":"aec9e99a-3163-4085-ed39-ec5679351138"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('인간', 'NNG'), ('이', 'MM'), ('컴퓨터', 'NNG'), ('오', 'VV'), ('아', 'EC'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'MM'), ('있', 'VV'), ('달', 'VV'), ('는', 'ETM'), ('것', 'NNB'), ('을', 'NNG'), ('깨닫', 'VV'), ('지', 'NNB'), ('못', 'MAG'), ('하', 'MAG'), ('고', 'MM'), ('인간', 'NNG'), ('과', 'NNG'), ('대화', 'NNG'), ('를', 'JKO'), ('계속', 'MAG'), ('하', 'NNG'), ('ㄹ', 'NA'), ('수', 'NNB'), ('있', 'VV'), ('다면', 'NNG'), ('컴퓨터', 'NNG'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('지능', 'NNP'), ('적', 'NNB'), ('이', 'MM'), ('ㄴ', 'JX'), ('것', 'NNB'), ('으로', 'JKB'), ('간주', 'NNG'), ('되', 'NNB'), ('ㄹ', 'NA'), ('수', 'NNB'), ('있', 'VV'), ('습니다', 'EC'), ('.', 'SF')]\n"]}],"source":["# 코모란(Komoran) 품사 태깅\n","komoranTag = []\n","for token in komoran_tokens:\n","    komoranTag += komoran.pos(token)\n","print(komoranTag)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1690702990574,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"msdBCzI6iA2w","outputId":"f73b71b2-7982-4ddc-8f0c-8ea4c6926c32"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('인간', 'N'), ('이', 'M'), ('컴퓨터', 'N'), ('와', 'I'), ('대화', 'N'), ('하', 'P'), ('고', 'E'), ('있', 'N'), ('다', 'M'), ('는', 'J'), ('것', 'N'), ('을', 'N'), ('깨닫', 'N'), ('지', 'N'), ('못하', 'P'), ('어', 'E'), ('고', 'M'), ('인간', 'N'), ('과', 'N'), ('대화', 'N'), ('를', 'N'), ('계속', 'M'), ('하', 'I'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('다면', 'N'), ('컴퓨터', 'N'), ('늘', 'P'), ('ㄴ', 'E'), ('지능적', 'N'), ('이', 'M'), ('ㄴ', 'N'), ('것', 'N'), ('으', 'N'), ('로', 'J'), ('간주', 'N'), ('되', 'N'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('슬', 'P'), ('ㅂ니다', 'E'), ('.', 'S')]\n"]}],"source":["# 한나눔(Hannanum) 품사 태깅\n","hannanumTag = []\n","for token in hannanum_tokens:\n","    hannanumTag += hannanum.pos(token)\n","print(hannanumTag)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1690702990574,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"dpe14zC3iCFi","outputId":"229bbdef-cd47-41cc-cd5b-cc8a01d127b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('인간', 'Noun'), ('이', 'Noun'), ('컴퓨터', 'Noun'), ('와', 'Verb'), ('대화', 'Noun'), ('하고', 'Verb'), ('있다는', 'Adjective'), ('것', 'Noun'), ('을', 'Josa'), ('깨닫지', 'Verb'), ('못', 'Noun'), ('하고', 'Verb'), ('인간', 'Noun'), ('과', 'Noun'), ('대화', 'Noun'), ('를', 'Noun'), ('계속', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있다면', 'Adjective'), ('컴퓨터', 'Noun'), ('는', 'Verb'), ('지능', 'Noun'), ('적', 'Noun'), ('인', 'Noun'), ('것', 'Noun'), ('으로', 'Josa'), ('간주', 'Noun'), ('될', 'Verb'), ('수', 'Noun'), ('있습니다', 'Adjective'), ('.', 'Punctuation')]\n"]}],"source":["# Okt 품사 태깅\n","oktTag = []\n","for token in okt_tokens:\n","    oktTag += okt.pos(token)\n","print(oktTag)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690702990574,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"xNQBKdYaiDd0","outputId":"b3341197-0d78-4e41-e782-1980a98ae19c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('인간', 'NNG'), ('이', 'NNG'), ('컴퓨터', 'NNG'), ('오', 'VA'), ('아', 'ECS'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'NNG'), ('있', 'VA'), ('달', 'VV'), ('는', 'ETD'), ('것', 'NNB'), ('을', 'NNG'), ('깨닫', 'VV'), ('지', 'NNG'), ('못하', 'VX'), ('고', 'NNG'), ('인간', 'NNG'), ('과', 'NNG'), ('대화', 'NNG'), ('를', 'UN'), ('계속', 'MAG'), ('하', 'NNG'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('다면', 'NNG'), ('컴퓨터', 'NNG'), ('늘', 'VA'), ('ㄴ', 'ETD'), ('지능', 'NNG'), ('적', 'NNG'), ('이', 'NNG'), ('ㄴ', 'NNG'), ('것', 'NNB'), ('으', 'UN'), ('로', 'JKM'), ('간주', 'NNG'), ('되', 'VA'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('슬', 'VV'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n"]}],"source":["# Kkma 품사 태깅\n","kkmaTag = []\n","for token in kkma_tokens:\n","    kkmaTag += kkma.pos(token)\n","print(kkmaTag)"]},{"cell_type":"markdown","metadata":{"id":"VZY4s8tbuuXP"},"source":["## 2.3 불용어(Stopword) 처리\n","분석에 불필요한 품사를 제거하고, 불필요한 단어(불용어)를 제거한다"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690702990575,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"Nvjk1yIYkCfj"},"outputs":[],"source":["#불용어 처리\n","stopPos = ['Suffix','Punctuation','Josa','Foreign','Alpha','Number']"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":2518,"status":"ok","timestamp":1690702993089,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"573iqrTFkcJ3"},"outputs":[],"source":["# 최빈어 조회. 최빈어를 조회하여 불용어 제거 대상을 선정\n","from collections import Counter\n","#Counter(oktTag).most_common()"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1690702993089,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"5lBkhHm1kYcz"},"outputs":[],"source":["stopWord = ['의','이','로','두고','들','를','은','과','수','했다','것','있는','한다','하는','그','있다','할','이런','되기','해야','있게','여기']"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1690702993090,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"BJgERpoikh9s"},"outputs":[],"source":["stopWord = ['의','이','로','두고','들','를','은','과','수','했다','것','있는','한다','하는','그','있다','할','이런','되기','해야','있게','여기']\n","word = []\n","for tag in oktTag:\n","    if tag[1] not in stopPos:\n","        if tag[0] not in stopWord:\n","            word.append(tag[0])"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1690702993090,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"iUQTDj4KkkBN","outputId":"84db6b4b-fd79-4820-adb9-19eb59634fa0"},"outputs":[{"name":"stdout","output_type":"stream","text":["['인간', '컴퓨터', '와', '대화', '하고', '있다는', '깨닫지', '못', '하고', '인간', '대화', '계속', '있다면', '컴퓨터', '는', '지능', '적', '인', '간주', '될', '있습니다']\n"]}],"source":["print(word)"]},{"cell_type":"markdown","metadata":{"id":"n1BDV2EAzug6"},"source":["# 3 N-gram\n","\n","- bigrams() : 토큰을 2개 단위로 묶음\n","- ngrams(tokens, n) : 토큰들을 n개 단위로 묶음"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1690702993090,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"U-X4misXzz6K","outputId":"6d18a6e7-8467-4620-d5fc-d47ac7953c16"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","from nltk import bigrams, word_tokenize\n","from nltk.util import ngrams\n","nltk.download('punkt')\n","\n","sentence = \"I am a boy.\"\n","tokens = word_tokenize(sentence)\n","\n","bigram = bigrams(tokens)\n","trigram = ngrams(tokens, 3)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690702993497,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"Cv-grZ4tz7gR","outputId":"6a50adba-fe47-4c96-ae6b-23e1fecd8aef"},"outputs":[{"name":"stdout","output_type":"stream","text":["('I', 'am')\n","('am', 'a')\n","('a', 'boy')\n","('boy', '.')\n"]}],"source":["for t in bigram:\n","    print(t)"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1690702993497,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"qco3k57t0HD8","outputId":"ec5eccdd-41a3-42ad-f75c-b11e3ad3ee34"},"outputs":[{"name":"stdout","output_type":"stream","text":["('I', 'am', 'a')\n","('am', 'a', 'boy')\n","('a', 'boy', '.')\n"]}],"source":["for t in trigram:\n","    print(t)"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4638,"status":"ok","timestamp":1690702998132,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"BD9vb0fN0XF3","outputId":"af116d46-1a0d-4531-db47-0921dfad25ed"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package movie_reviews to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["[('SS', 'plot'),\n"," ('plot', ':'),\n"," (':', 'two'),\n"," ('two', 'teen'),\n"," ('teen', 'couples'),\n"," ('couples', 'go'),\n"," ('go', 'to'),\n"," ('to', 'a'),\n"," ('a', 'church'),\n"," ('church', 'party'),\n"," ('party', ','),\n"," (',', 'drink'),\n"," ('drink', 'and'),\n"," ('and', 'then'),\n"," ('then', 'drive'),\n"," ('drive', '.'),\n"," ('.', 'SE'),\n"," ('SS', 'they'),\n"," ('they', 'get'),\n"," ('get', 'into')]"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('movie_reviews')\n","nltk.download('punkt')\n","from nltk.corpus import movie_reviews\n","\n","sentences = []\n","for tokens in movie_reviews.sents():\n","    bigram = ngrams(tokens, 2, pad_left=True, pad_right=True, left_pad_symbol=\"SS\", right_pad_symbol=\"SE\")\n","    sentences += [t for t in bigram]\n","\n","sentences[:20]"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1690702998132,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"prEVvkXVN47C","outputId":"973846d4-2358-4893-b8cd-c78f73c99dd5"},"outputs":[{"data":{"text/plain":["[['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.'], ...]"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["movie_reviews.sents()"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1690702998133,"user":{"displayName":"최재진","userId":"14104918437285699837"},"user_tz":-540},"id":"-LDudkdd7ogY","outputId":"ca471703-1428-4c53-f596-0c0a1bc04e46"},"outputs":[{"data":{"text/plain":["[('whatever', 'it'), ('it', 'may'), ('may', 'be'), ('be', '.'), ('.', 'SE')]"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["sentences[-5:]"]}],"metadata":{"colab":{"provenance":[{"file_id":"1JMXIcuvjMt9II5oBMFmRrzh0DrBI2Bk_","timestamp":1594615822602}]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"e508c96042fd7b3aa969c1a8875668ac50b0a6c54de6b2bab6d59ac763cd3db2"}}},"nbformat":4,"nbformat_minor":0}
